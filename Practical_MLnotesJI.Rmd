---
title: "Practical Machine Learning"
author: "Julie I. Santos"
date: "Tuesday, April 07, 2015"
output: html_document
---

## Table of Contents

* [Assignment](#PA)
* [Week 1](#Lectures1)
* [Week 2](#Lectures2)
* [Week 3](#Lectures3)
* [Homeworks](#HW)

<a id="PA">Peer Assessment:</a>

###Write up

**Background**

In this project, your goal will be to use data from accelerometers on the belt, forearm, arm and dumbbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways.  More information is available at: <a href="http://groupware.les.inf.puc-rio.br/har">here</a> (see the section on the Weight Lifting Exercise Dataset).

**Data**

The training data for this project are available here:

<a href="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv">https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv</a>

The test data are available here:

<a href="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv">https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv</a>

The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har. If you use the document you create for this class for any purpose please cite them as they have been very generous in allowing their data to be used for this kind of assignment. 

**What you should submit**

The goal of your project is to predict the manner in which they did the exercise.  This is the "classe" variable in the training set.  You may use any of the other variables to predict with. You should create a report describing

* How you built your model
* How you used cross validation
* what you think the expected out of sample error is
* and why you made the choices you did

You will also use your prediction model to predict 20 different test cases.

1.  Your submission should consist of a link to a Github repo with your R markdown and compiled HTML file describing your analysis. Please constrain the text of the writeup to < 2000 words and the number of figures to < 5.  It will make it easier for the graders if you submit a repo with a gh-pages branch so the HTML page can be viewed online.
2.  You should also apply your machine learning algorithm to the 20 test cases available in the test data above.  Please submit your predictions in appropriate format to the programming assignment for automated grading.

**Reproducibility**

Please be sure that if an evaluator downloads the repo, they will be able to view the compiled HTML version of your analysis.


**Evaluation**

1. Has the student submitted a github repo?
2. Does the submission build a machine learning algorithm to predict activity quality from activity monitors? To evaluate the HTML file you may have to download the repo and open the compiled HTML document. Alternatively, if they have submitted a repo with a gh-pages branch, you may be able to view the HTML page on the web.  If the repo is:

<a href="https://github.com/DataScienceSpecialization/courses/tree/master/08_PracticalMachineLearning/001predictionMotivation">https://github.com/DataScienceSpecialization/courses/tree/master/08_PracticalMachineLearning/001predictionMotivation</a>

then you can view the HTML page here:

<a href="http://datasciencespecialization.github.io/courses/08_PracticalMachineLearning/001predictionMotivation/">http://datasciencespecialization.github.io/courses/08_PracticalMachineLearning/001predictionMotivation/</a>

Here is the link to my page:
<a href="http://courserads-jis.github.io/PML_Course_Project/">http://courserads-jis.github.io/PML_Course_Project/</a>

**REMEMBER**: When you push changes, you have to use *git push origin gh-pages*, not master.

3.  Do the authors describe what they expect the out of sample error to be and estimate the error appropriately with cross-validation?

4.  Please use the space below to provide constructive feedback...


### Course Project: Submission instructions

Please apply the machine learning algorithm you built to each of the 20 test cases in the testing data set.  For more information and instructions on how to build your model see the prediction assignment write-up. For each test case you should submit a text file with a single capital letter (A, B, C, D, or E) corresponding to your prediction for the corresponding problem in the test data set.  You get one point for each correct answer.   You may submit up to 2 times for each problem. It may be helpful to use the following function to create the files. If you have a character vector with your 20 predictions in order for the 20 problems, so something like:

```{r eval=FALSE}
answers <- rep("A",20)
```

then you can load this function by copying it and pasting it into R:

```{r eval=FALSE}
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
```

then create a folder where you want the files to be written. Set that to be your working directory and run:

```{r eval=FALSE}
pml_write_files(answers)
```

and it will create one file for each submission. Note: if you use this script, please make sure the files that get written out have one character each with your prediction for the corresponding problem ID. I have noticed the script produces strange results if the answers variable is not a character vector.


<a id="Lectures1">Week 1</a>

Start with a question. i.e. can we predict whether an email is spam or not?

Get some data. i.e. content of emails.

Compress data into a set of features. i.e. count of word "your" in email.

Feed features into an algorithm.
i.e. if frequency of the word "your" > 0.5, label email as spam.

###In Sample and Out of Sample Errors

**In Sample Error**: The error rate you get on the same data set you used to build your predictor.  Also called *resubstitution error*.

**Out of Sample Error**: The error rate you get on a new data set. Also called *gneralization error*.

**Key ideas**

1. Out of sample error is what you care about.
2. In sample error < out of sample error
3. The reason is overfitting
    * Matching your algorithm to the data you have.
  
```{r}
library(kernlab)
data(spam)
set.seed(333)
smallSpam <-spam[sample(dim(spam)[1], size=10),]
spamLabel <- (smallSpam$type=="spam")*1 + 1
plot(smallSpam$capitalAve,col=spamLabel)
```
So, we're taking a small sample (10) of the data set and looking at the average number of capital letters.

So, let's make a model:

* It looks like if we set the threshold between 2.4 and 2.7, anything above 2.7 will be labeled "spam", anything below 3.4 "nonspam", we'll get most of the emails right.

* But, to get them all right, let's add this caveat: if capitalAve falls between 2.4 and 2.45, it's "spam", if it's between 2.45 and 2.7 it's "non-spam".  That will predict our data perfectly (**perfect accuracy**).

* It turns out that when you just apply the first rule and not the caveat to the full dataset, you get a better fit.  The reason is that you're fitting both signal and noise when you're going for perfect accuracy. 

```{r}
rule1 <- function(x) {
  prediction <- rep(NA,length(x))
  prediction[x > 2.7] <- "spam"
  prediction[x < 2.4] <- "nospam"
  prediction[( x >= 2.4 & x <=2.45)] <- "spam"
  prediction[(x>2.45 & x <=2.70)] <- "nospam"
  return(prediction)
}
table(rule1(smallSpam$capitalAve),smallSpam$type)

rule2 <- function(x) {
  prediction <- rep(NA,length(x))
  prediction[x > 2.8] <- "spam"
  prediction[x <= 2.8] <- "nospam"
  return(prediction)
}
table(rule2(smallSpam$capitalAve),smallSpam$type)

table(rule1(spam$capitalAve),spam$type)
table(rule2(spam$capitalAve), spam$type)
sum(rule1(spam$capitalAve)==spam$type)
sum(rule2(spam$capitalAve)==spam$type)
```

Well those sums didn't give us what the slides did, but, anyway, the point is that the second rule, does better at predicting outcomes in the entire data set.

### What's going on?

<center style="color:red">Overfitting</center>

* Data have two parts
  
    - Signal
    
    - Noise
    
* The goal of a predictor is to find the signal

* You can always design a perfect in-sample predictor

* You capture both the signal + noise when you do that

* Predictor won't perform as well on new samples

## Prediction Study Design

*how to minimize the problems that can be caused by in sample vs. out of sample error rate.*

1. Define your error rate 

  You can use several different

2. Split data into
  
  - Training, testing, and validation (optional)
  
3. On the training set, pick features

  - Use cross-validation
  
4. On the training set, pick prediction function

  - Use cross-validation
  
5. If no validation set

  - Apply the best model 1x to test set (don't use the test set to train more)
  
6. If validation set and test set

  - Apply to test set and refine
  
  - Apply 1x to validation set
  
  
### Know the benchmarks

They do this in Kaggle competitions. You'll see benchmark models that you can measure your model against.

## Avoid small samples

They'll give you a false sense of accuracy.

## Rules of thumb for prediction study design

- If you have a large sample size
  
    - 60% training, 20% test, 20% validation
    
- If you have a medium sample size

    - 60% training, 40% testing
    
- If you have a small sample size

    - Do cross validation
    
    - Report caveat of small sample size
    
##  Principles to remember

- Test/validation set should be set aside and never looked at.

- In general, *randomly* sample training and test

- Your data sets must reflect structure of the problem

  - If predictions evolve with time, split train/test in time chunks (called *backtesting* in finance)
  
- All subsets should reflect as much diversity as possible


  - Random assignment does this
  
  - You can try to balance by features, but this is tricky.

## Types of Errors

### Basic terms

**For discrete data, or binary data, there are only two types of errors you can make: false positives and false negatives**

True positive, true negative, false positive, false negative.


**Sensitivity** : Pr(positive test | disease ) : TP / (TP + FN)

**Specificity** : Pr(negative test | no disease) : TN / (FP + TN)

**Positive predictive value** : Pr(disease | positive test) : TP / (TP + FP)

**Negative predictive value** : Pr(no disease | negative test) : TN / (FN + TN)


**Accuracy** : Pr( correct outcome ) : (TP + TN)/(TP + FP + FN + TN)


## Continuous Data

**For continuous data, the goal is to see how close you are to the truth**

*Mean squared error (MSE)*

$$ \frac{1}{n}\sum_{i=1}^{n} (Prediction_i - Truth_i)^2 $$

the average distance between the prediction and truth.

*Root mean squared error (RMSE)*

$$ \sqrt{\frac{1}{n}\sum_{i=1}^{n} (Prediction_i - Truth_i)^2} $$

These measures are sensitive to outliers.


## Common error measures

1.  Mean squared error and RMSE
  
  * Sensitive to outliers
  
2. Median absolute error

  * More robust to outliers
  
3. Sensitivity
  
  * If you want few missed positives
  
4. Specificity

  * If you want few negatives called positives
  
5. Accuracy

  * Weights false positives/negatives equally
  
6. Concordance: for multi-class data, it's a distance measurement.

  * One example is Kappa


## ROC curves

*Receiver Operating Characteristic* curves are a commonly used measure of the quality of a prediction algorithm.

### Why a curve?

In binary data, you often get quantitative data, that is the binary outcomes are expressed as probabilities of success vs. probability of failure.  The cutoff you choose will give different results and have different properties.

ROC curves plot on the x-axis 1-specificity, or the probability of a false positive. On the y-axis the sensitivity or probability of a true positive.

![ROC curve](P1.PNG)

So, every point on the curve is a different choice of cutoff. So, you would plot several ROC curves for each algorithm you test and compare.    

![ROC curve](P2.PNG)

Calculate the area under the curve to assess which algorithm is better.

###Area under the curve (AUC)

* If the area under the curve equals 0.5 (45 degree line), that's as good as random guessing.

* If the area is 1, that's a perfect classifier.

* In general, if the AUC is > 0.8, we consider this to be a good classifier.

The closer you are to the top left of the plot, the better the algorithm.

##Cross validation

This is used to help you minimize overfitting to the training set.

*Approach*

1. Take the training set

2. Split it into training/test sets

3. Build a model on the training set

4. Evaluate on the test set

5. Repeat and average the estimated errors

*Used for*

1. Picking variables to include in a model

2. Picking the type of prediction function to use

3. Picking the parameters in the prediction function

4. Comparing different predictors.

You can split the training set up into sub-training test sets by random subsampling, K-fold cross-validation, Leave one out

##Considerations

* For time series data, the samples must be in chunks (autocorrelation)

* For k-fold cross validation

  - Larger k = less bias, more variance (will depend a lot on which subsample you use)
  
  - Smaller k = more bias, less variance
  
* Random sampling must be done *without replacement*

* Random sampling with replacement is called *bootstrap*

  - Underestimates the error (some samples will appear more than once and if you get the prediction right for one, you get it right for the duplicated one)
  
  - Can be corrected but it's complicated (0.632 Bootstrap)
  
* If you cross-validate to pick predictors, you must estimate errors on independent data.

##What data should you use?

As often as possible, use like data to predict like and when you're using data that isn't related, be very careful about interpreting why your algorithm works or doesn't work.


<a id="Lectures2">Week 2</a>

##The caret package

###Caret functionality

* Some preprocessing and cleaning

  - preProcess
  
* Data splitting

  - createDataPartition
  
  - createResample
  
  - createTimeSlices
  
* Training/testing functions

  - train
  
  - predict
  
* Model comparison

  - confusionMatrix
  
###Machine learning algorithms in R
 
* Linear discriminant analysis

* Regression

* Naive Bayes

* Support vector machines

* Classification and regression trees

* Random forests

* Boosting

* etc.

## Example of Data Splitting

```{r}
library(caret); library(kernlab); data(spam)
inTrain <- createDataPartition(y=spam$type, p = 0.75, list=FALSE)
training <- spam[inTrain,]
testing <- spam[-inTrain,]
dim(training)
```

### Cross-validation

Split the training set into subsets

```{r}
set.seed(32323)
folds <- createFolds(y=spam$type, k=10, list=TRUE, returnTrain=TRUE)

sapply(folds,length)
folds[[1]][1:10] # the fold seems to be a list of numerics corresponding to the indices that will be in that fold
```

So, then you would probably use *folds[[x]]* to subset the original data.

### Resampling

This is with replacement, so bootstrap.

```{r}
set.seed(32323)
folds <- createResample(y=spam$type, times=10, list=TRUE)
sapply(folds, length)
folds[[1]][1:10]
```

### Time slices

```{r}
set.seed(32323)
tme <- 1:1000 # this is basically the vector that you're creating the slices on.
folds <- createTimeSlices(y=tme, initialWindow=20, horizon=10)
names(folds)
folds$train[[1]]
folds$test[[1]]
```

## Training Options

```{r warning=FALSE}
library(caret); library(kernlab); data(spam)
inTrain <- createDataPartition(y=spam$type, p=0.75, list=FALSE)

training <- spam[inTrain, ]
testing <- spam[-inTrain, ]
modelFit <- train(type ~.,data=training, method="glm")
args(train.default)
```

### Metric Options

**Continuous outcomes**

* *RMSE*: Root Mean Squared Error

* *$R^2$*: measure of linear aggreement between the variables you're predicting and the variables you predict with (not great if you're doing non-linear things)

**Categorical Outcomes**

* *Accuracy*: fraction correctly predicted

* *Kappa*: a measure of concordance

```{r}
args(trainControl)
```

### trainControl resampling

* *method*

  - boot = bootstrapping
  
  - boot632 = bootstrapping with adjustment
  
  - cv = cross validation
  
  - repeated cv = repeated cross validation
  
  - LOOCV = leave one out cross validation
  
* *number*

  - For boot/cross validation
  
  - Number of subsamples to take
  
* *repeats*

  - Number of times to repeat subsampling
  
  - If large, this can *slow thing down*
  

## Plotting Predictors

### Example: predicting wages

```{r}
library(ISLR); library(ggplot2); library(caret)

data(Wage)
summary(Wage)
inTrain <- createDataPartition(y=Wage$wage, p=0.7, list=FALSE)
training <- Wage[inTrain,]
testing <- Wage[-inTrain,]
dim(training);dim(testing)
```

### Feature plot, from the caret package
```{r}
featurePlot(x=training[,c("age","education","jobclass")], y= training$wage, plot="pairs")
qplot(age,wage,data=training)
```

There is something weird going on between age and wage as evidenced by the band on top. So, let's color the types of job classes.

```{r}
qplot(age,wage,colour=jobclass, data=training)
```

So, that seems to be coming mostly from the information based jobs as opposed to the industrial-based jobs.

###Add regression smoothers, from ggplot2 package

```{r}
qq <- qplot(age, wage, colour=education, data=training)
qq + geom_smooth(method='lm', formula=y~x)
```

###cut2, making factors from the Hmisc package

You might want to split the wage variable into different categories

```{r warning=FALSE, message=FALSE}
library(Hmisc)
cutWage <- cut2(training$wage, g=3)
table(cutWage)
```

This breaks the dataset up into factors based on the quantile groups. Above, we told cut2 that we want 3 groups.  This can be used to make different kinds of plots.

```{r}
library(gridExtra)
p1 <- qplot(cutWage, age, data=training, fill=cutWage, geom=c("boxplot"))
p2 <- qplot(cutWage,age,data=training,fill=cutWage, geom=c("boxplot","jitter"))
grid.arrange(p1,p2,ncol=2)
```

